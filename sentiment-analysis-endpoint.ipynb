{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis with TensorFlow\n",
    "\n",
    "Sentiment analysis is a very common text analytics task that involves determining whether a text sample is positive or negative about its subject.  There are several different algorithms for performing this task, including statistical algorithms and deep learning algorithms.  With respect to deep learning, a Convolutional Neural Net (CNN) is sometimes used for this purpose.  In this notebook we'll use a CNN built with TensorFlow to perform sentiment analysis in Amazon SageMaker on the IMDB dataset, which consists of movie reviews labeled as having positive or negative sentiment. Three aspects of Amazon SageMaker will be demonstrated:\n",
    "\n",
    "- How to use Script Mode with a prebuilt TensorFlow container, along with a training script similar to one you would use outside SageMaker. \n",
    "- Local Mode training, which allows you to test your code on your notebook instance before creating a full scale training job.\n",
    "- Batch Transform for offline, asynchronous predictions on large batches of data. \n",
    "\n",
    "#  Prepare Dataset\n",
    "\n",
    "We'll begin by loading the reviews dataset, and padding the reviews so all reviews have the same length.  Each review is represented as an array of numbers, where each number represents an indexed word.  Training data for both Local Mode and Hosted Training must be saved as files, so we'll also save the transformed data to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() # we are using the notebook instance role for training in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "csv_test_dir = os.path.join(os.getcwd(), 'data/csv-test')\n",
    "os.makedirs(csv_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame(x_train).to_csv(os.path.join(train_dir, 'x_train.csv'), header=None, index=False)\n",
    "pd.DataFrame(y_train).to_csv(os.path.join(train_dir, 'y_train.csv'), header=None, index=False)\n",
    "pd.DataFrame(x_test).to_csv(os.path.join(test_dir, 'x_test.csv'), header=None, index=False)\n",
    "pd.DataFrame(y_test).to_csv(os.path.join(test_dir, 'y_test.csv'), header=None, index=False)\n",
    "np.savetxt(os.path.join(csv_test_dir, 'csv-test.csv'), np.array(x_test[:100], dtype=np.int32), fmt='%d', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Mode Training\n",
    "\n",
    "Amazon SageMaker’s Local Mode training feature is a convenient way to make sure your code is working as expected before moving on to full scale, hosted training. With Local Mode, you can run quick tests with just a sample of training data, and/or a small number of epochs (passes over the full training set), while avoiding the time and expense of attempting full scale hosted training using possibly buggy code.  \n",
    "\n",
    "### Setup for Local Mode\n",
    "\n",
    "To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU) installed in the notebook instance. Running the following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-docker2 already installed. We are good to go!\n",
      "SageMaker instance route table setup is ok. We are good to go.\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash ./local_mode/local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Estimator\n",
    "\n",
    "The next step is to set up a TensorFlow Estimator for Local Mode training. A key parameters for the Estimator is the `train_instance_type`, which is the kind of hardware on which training will run. In the case of Local Mode, we simply set this parameter to `local_gpu` to invoke Local Mode training on the GPU, or to `local` if the instance has a CPU. Other parameters of note are the algorithm’s hyperparameters, which are passed in as a dictionary, and a Boolean parameter indicating that we are using Script Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'local'\n",
    "hyperparameters = {'epochs': 1, 'batch_size': 128}\n",
    "local_estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='./training_scripts/',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='tf-keras-sentiment',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll briefly train the model in Local Mode.  Since this is just to make sure the code is working, we'll train for only one epoch.  (Note that on a CPU-based notebook instance, this one epoch will take at least 3 or 4 minutes.)  As you'll see from the logs below the cell when training is complete, even when trained for only one epoch, the accuracy of the model on training data is already at almost 80%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpiqrefcsd_algo-1-ri42s_1 ... \n",
      "\u001b[1BAttaching to tmpiqrefcsd_algo-1-ri42s_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:13,458 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:13,470 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:13,646 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m /usr/local/bin/python3.6 -m pip install -U . -r requirements.txt\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Collecting scikit-learn (from -r requirements.txt (line 1))\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 13.4MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.4)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Collecting joblib>=0.11 (from scikit-learn->-r requirements.txt (line 1))\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 44.7MB/s eta 0:00:01\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.2.1)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Building wheels for collected packages: sagemaker-keras-example\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m   Building wheel for sagemaker-keras-example (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-gat7tis1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Successfully built sagemaker-keras-example\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Installing collected packages: joblib, scikit-learn, sagemaker-keras-example\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m   Found existing installation: scikit-learn 0.20.3\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     Uninstalling scikit-learn-0.20.3:\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m       Successfully uninstalled scikit-learn-0.20.3\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Successfully installed joblib-0.14.1 sagemaker-keras-example-1.0 scikit-learn-0.22.2.post1\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \u001b[33mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:20,429 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:20,489 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:20,541 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:37:20,579 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"current_host\": \"algo-1-ri42s\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"algo-1-ri42s\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"batch_size\": 128,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"job_name\": \"tf-keras-sentiment-2020-04-17-11-37-08-007\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"master_hostname\": \"algo-1-ri42s\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-37-08-007/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"current_host\": \"algo-1-ri42s\",\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m             \"algo-1-ri42s\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_HOSTS=[\"algo-1-ri42s\"]\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_HPS={\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\"}\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ri42s\",\"hosts\":[\"algo-1-ri42s\"]}\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_CURRENT_HOST=algo-1-ri42s\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-37-08-007/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-ri42s\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-ri42s\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":1,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-keras-sentiment-2020-04-17-11-37-08-007\",\"log_level\":20,\"master_hostname\":\"algo-1-ri42s\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-37-08-007/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ri42s\",\"hosts\":[\"algo-1-ri42s\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"1\",\"--model_dir\",\"/opt/ml/model\"]\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_HP_BATCH_SIZE=128\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m /usr/local/bin/python3.6 -m train --batch_size 128 --epochs 1 --model_dir /opt/ml/model\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Using TensorFlow backend.\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m x train (25000, 400) y train (25000, 1)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m x test (25000, 400) y test (25000, 1)\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m Train on 25000 samples, validate on 25000 samples\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m  - 186s - loss: 0.4303 - acc: 0.7871 - val_loss: 0.3001 - val_acc: 0.8696\n",
      "\u001b[36malgo-1-ri42s_1  |\u001b[0m 2020-04-17 11:40:35,836 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpiqrefcsd_algo-1-ri42s_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "inputs = {'train': f'file://{train_dir}',\n",
    "          'test': f'file://{test_dir}'}\n",
    "\n",
    "local_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hosted Training\n",
    "\n",
    "After we've confirmed our code seems to be working using Local Mode training, we can move on to use SageMaker's hosted training, which uses compute resources separate from your notebook instance.  Hosted training spins up one or more instances (cluster) for training, and then tears the cluster down when training is complete. In general, hosted training is preferred for doing actual training, especially for large-scale, distributed training. Before starting hosted training, the data must be present in storage that can be accessed by SageMaker. The storage options are:  Amazon S3 (object storage service), Amazon EFS (elastic NFS file system service), and Amazon FSx for Lustre (high-performance file system service). For this example, we'll upload the data to S3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://mlops-bucket-366243680492/data/train', 'test': 's3://mlops-bucket-366243680492/data/test'}\n"
     ]
    }
   ],
   "source": [
    "bucket = 'mlops-bucket-366243680492'\n",
    "\n",
    "traindata_s3_prefix = 'data/train'\n",
    "testdata_s3_prefix = 'data/test'\n",
    "\n",
    "train_s3 = sagemaker_session.upload_data(path='./data/train/', bucket=bucket, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sagemaker_session.upload_data(path='./data/test/', bucket=bucket, key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training data now in S3, we're ready to set up an Estimator object for hosted training. It is similar to the Local Mode Estimator, except the `train_instance_type` has been set to a ML instance type instead of a local type for Local Mode. Additionally, we've set the number of epochs to a number greater than one for actual training, as opposed to just testing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "hyperparameters = {'epochs': 10, 'batch_size': 128}\n",
    "model_dir = '/opt/ml/model'\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='./training_scripts/',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       base_job_name='tf-keras-sentiment',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the change in training instance type and increase in epochs, we simply call `fit` to start the actual hosted training.  At the end of hosted training, you'll see from the logs below the cell that accuracy on the training set has greatly increased, and accuracy on the validation set is around 90%.  The model may be overfitting now (less able to generalize to data it has not yet seen), even though we are employing dropout as a regularization technique.  In a production situation, further investigation would be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-17 11:44:53 Starting - Starting the training job...\n",
      "2020-04-17 11:45:13 Starting - Launching requested ML instances......\n",
      "2020-04-17 11:46:10 Starting - Preparing the instances for training......\n",
      "2020-04-17 11:47:15 Downloading - Downloading input data...\n",
      "2020-04-17 11:47:46 Training - Downloading the training image...\n",
      "2020-04-17 11:48:06 Training - Training image download completed. Training in progress.\u001b[34m2020-04-17 11:48:10,133 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-04-17 11:48:10,547 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.2.1)\u001b[0m\n",
      "\u001b[34mCollecting joblib>=0.11 (from scikit-learn->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker-keras-example\n",
      "  Building wheel for sagemaker-keras-example (setup.py): started\n",
      "  Building wheel for sagemaker-keras-example (setup.py): finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5p0fb1v3/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker-keras-example\u001b[0m\n",
      "\u001b[34mInstalling collected packages: joblib, scikit-learn, sagemaker-keras-example\n",
      "  Found existing installation: scikit-learn 0.20.3\u001b[0m\n",
      "\u001b[34m    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\u001b[0m\n",
      "\u001b[34mSuccessfully installed joblib-0.14.1 sagemaker-keras-example-1.0 scikit-learn-0.22.2.post1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-17 11:48:15,095 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-keras-sentiment-2020-04-17-11-44-53-317\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-44-53-317/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":10,\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-44-53-317/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":10,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-keras-sentiment-2020-04-17-11-44-53-317\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-366243680492/tf-keras-sentiment-2020-04-17-11-44-53-317/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"10\",\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m train --batch_size 128 --epochs 10 --model_dir /opt/ml/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mx train (25000, 400) y train (25000, 1)\u001b[0m\n",
      "\u001b[34mx test (25000, 400) y test (25000, 1)\u001b[0m\n",
      "\u001b[34mTrain on 25000 samples, validate on 25000 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/10\u001b[0m\n",
      "\u001b[34m - 7s - loss: 0.4304 - acc: 0.7841 - val_loss: 0.2854 - val_acc: 0.8786\u001b[0m\n",
      "\u001b[34mEpoch 2/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 0.1726 - acc: 0.9350 - val_loss: 0.2432 - val_acc: 0.9018\u001b[0m\n",
      "\u001b[34mEpoch 3/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 0.0522 - acc: 0.9850 - val_loss: 0.2887 - val_acc: 0.8978\u001b[0m\n",
      "\u001b[34mEpoch 4/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 0.0101 - acc: 0.9986 - val_loss: 0.3185 - val_acc: 0.9019\u001b[0m\n",
      "\u001b[34mEpoch 5/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.3457 - val_acc: 0.9025\u001b[0m\n",
      "\u001b[34mEpoch 6/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 8.6250e-04 - acc: 1.0000 - val_loss: 0.3665 - val_acc: 0.9028\u001b[0m\n",
      "\u001b[34mEpoch 7/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 4.3225e-04 - acc: 1.0000 - val_loss: 0.3821 - val_acc: 0.9028\u001b[0m\n",
      "\u001b[34mEpoch 8/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 3.0471e-04 - acc: 1.0000 - val_loss: 0.3943 - val_acc: 0.9027\u001b[0m\n",
      "\u001b[34mEpoch 9/10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m - 4s - loss: 2.0661e-04 - acc: 1.0000 - val_loss: 0.4049 - val_acc: 0.9029\u001b[0m\n",
      "\u001b[34mEpoch 10/10\u001b[0m\n",
      "\u001b[34m - 4s - loss: 1.7933e-04 - acc: 1.0000 - val_loss: 0.4148 - val_acc: 0.9029\u001b[0m\n",
      "\u001b[34m2020-04-17 11:49:06,750 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-17 11:49:14 Uploading - Uploading generated training model\n",
      "2020-04-17 11:49:14 Completed - Training job completed\n",
      "Training seconds: 119\n",
      "Billable seconds: 119\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosted Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "sentiment_predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'^[\\?\\s]+')\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspired by hitchcock's strangers on a train concept of two men swapping murders in exchange for getting rid of the two people messing up their lives throw ? from the train is an original and very inventive comedy take on the idea it's a credit to danny devito that he both wrote and starred in this minor comedy gem br br anne ramsey is the mother who inspires the film's title and it's understandable why she gets under the skin of danny devito with her sharp tongue and relentlessly putting him down for any minor ? billy crystal is the writer who's wife has stolen his book idea and is now being ? as a great new author even appearing on the oprah show to in ? he should be enjoying thus devito gets the idea of swapping murders to rid themselves of these nuisance factors br br of course everything and anything can happen when writer carl reiner lets his imagination roam with ? ideas for how the plot develops and it's amusing all the way through providing plenty of laughs and chuckles along the way as well as a good deal of suspense br br for ? of black comedy this one is guaranteed to please\n"
     ]
    }
   ],
   "source": [
    "review_index = 10\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_test[review_index]])\n",
    "print(regex.sub('', decoded_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled sentiment for this review is positive, predicted sentiment is positive\n"
     ]
    }
   ],
   "source": [
    "results = sentiment_predictor.predict(x_test[review_index])['predictions'][0][0]\n",
    "\n",
    "def get_sentiment(score):\n",
    "    return 'positive' if score > 0.5 else 'negative' \n",
    "\n",
    "print('Labeled sentiment for this review is {}, predicted sentiment is {}'.format(get_sentiment(y_test[review_index]), \n",
    "                                                                                  get_sentiment(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(sentiment_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep learning models is a stochastic process, so your results may vary -- there is no guarantee that the predicted result will match the actual label. However, it is likely that the sentiment prediction agrees with the label for this review.  Let's now examine another review:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is likely (but not guaranteed) that the prediction agreed with the label for the test data.  Note that there is no need to clean up any Batch Transform resources:  after the transform job is complete, the cluster used to make inferences is torn down.\n",
    "\n",
    "Now that we've reviewed some sample predictions as a sanity check, we're finished.  Of course, in a typical production situation, the data science project lifecycle is iterative, with repeated cycles of refining the model using a tool such as Amazon SageMaker's Automatic Model Tuning feature, and gathering more data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
